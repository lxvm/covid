{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Requirements:\n",
    "\n",
    "Python 3.7+\n",
    "\n",
    "The packages below: IPython, ipywidgets, numpy, pandas, bokeh\n",
    "\n",
    "ipywidgets enabled  in jupyter notebook. This can be done on the command line with the two following commands:\n",
    "\n",
    "```\n",
    ">> jupyter nbextension install --py widgetsnbextension --user\n",
    "\n",
    ">> jupyter nbextension enable widgetsnbextension --user --py\n",
    "```\n",
    "Though if you are using jupyter lab your setup must include the following extensions\n",
    "```\n",
    ">> jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    ">> jupyter labextension install @bokeh/jupyter_bokeh\n",
    "```\n",
    "\n",
    "# COVID19 Data\n",
    "\n",
    "We will explore data sets from the New York Times about cases and deaths, Google data about mobility in different sectors of public life, and the aggregate mobility index from DescartesLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.layouts import row\n",
    "from bokeh.io import output_notebook, push_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import New York Times Data by US state and county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"http://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\",header=0, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.copy()\n",
    "df = df.query('fips>0')\n",
    "\n",
    "# df['date_processed'] = pd.to_datetime(df['date'].values)\n",
    "# df['date_processed'] = (df['date_processed'] - pd.Timestamp('2020-03-01')) / np.timedelta64(1, 'D')\n",
    "# df = df.query('date_processed>=0')\n",
    "\n",
    "# Imposing a clean index\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index([\"state\",\"county\",\"fips\",\"date\"],inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Google Mobility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_cols = [\"country_region_code\",\"sub_region_1\",\"sub_region_2\",\"date\",\"retail_and_recreation_percent_change_from_baseline\",\\\n",
    "           \"grocery_and_pharmacy_percent_change_from_baseline\",\"parks_percent_change_from_baseline\",\\\n",
    "           \"transit_stations_percent_change_from_baseline\",\"workplaces_percent_change_from_baseline\",\\\n",
    "           \"residential_percent_change_from_baseline\"]\n",
    "\n",
    "gg_df = pd.read_csv('https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv?',\n",
    "                    usecols=gg_cols,low_memory=False,parse_dates=['date'])\n",
    "locs=[\"Retail & recreation\",\"Grocery & pharmacy\",\"Parks\",\"Transit stations\",\"Workplaces\",\"Residential\"]\n",
    "gg_df.columns = [\"country\",\"state\",\"county\",\"date\"]+locs\n",
    "gg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing counties\n",
    "gg_df[\"county\"]=gg_df[\"county\"].str.replace(' County', '')\n",
    "\n",
    "def correct_county(row):\n",
    "    if(row.county=='New York'):\n",
    "        return \"New York City\"\n",
    "    else:\n",
    "        return row.county\n",
    "gg_df['county']=gg_df.apply(lambda row: correct_county(row), axis=1)\n",
    "\n",
    "# Formatting time\n",
    "gg_df['date'] = pd.to_datetime(gg_df['date'])\n",
    "\n",
    "# Restrict to the US states\n",
    "j_df=gg_df[gg_df[\"country\"]==\"US\"][pd.notna(gg_df[\"state\"])][pd.notna(gg_df[\"county\"])].copy()\n",
    "del j_df[\"country\"]\n",
    "\n",
    "# Sorting joinable dataset\n",
    "j_df=j_df.set_index([\"state\",\"county\",\"date\"]).sort_values([\"state\",\"county\",\"date\"])\n",
    "\n",
    "# Indexing as, and joining with main dataset\n",
    "df=df.join(j_df)\n",
    "\n",
    "# Handling missing data\n",
    "gg_df=gg_df[gg_df[\"country\"]==\"US\"][pd.notna(gg_df[\"state\"])].set_index([\"state\",\"date\"]).sort_values([\"state\",\"date\"])\n",
    "\n",
    "for state, state_df in df.groupby(level='state'):\n",
    "    for county, county_df in state_df.groupby(level='county'):\n",
    "        for loc in locs:\n",
    "            c_df=county_df[loc]\n",
    "            # If not enough data points, go fetch state data \n",
    "       #     if np.sum(c_df.count())<c_df.size/10:\n",
    "       #         c_df=pd.merge(c_df,gg_df[pd.isna(gg_df[\"county\"])].loc[state][loc],how=\"left\", \\\n",
    "       #                       left_index=True,right_index=True,suffixes=('_c',''))[loc]\n",
    "            df.loc[(state,county),loc]=c_df.interpolate().values\n",
    "    print(state, \"done !\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##download and integrate Descartes Labs mobility data\n",
    "des_df = pd.read_csv('https://raw.githubusercontent.com/descarteslabs/DL-COVID-19/master/DL-us-mobility-daterow.csv', \\\n",
    "                     usecols=['date','admin1','admin2','fips','m50_index'])\n",
    "des_df.columns = [\"date\",\"state\",\"county\",\"fips\",\"m50_index\"]\n",
    "\n",
    "# Correct DC\n",
    "def correct_state(row):\n",
    "    if(row.state=='Washington, D.C.'):\n",
    "        return \"District of Columbia\"\n",
    "    else:\n",
    "        return row.state\n",
    "des_df['state']=des_df.apply(lambda row: correct_state(row), axis=1)\n",
    "\n",
    "# Copy to join as before\n",
    "j_df = des_df.copy()\n",
    "del j_df[\"state\"],j_df[\"county\"]\n",
    "\n",
    "# Formatting time\n",
    "j_df[\"date\"] = pd.to_datetime(j_df[\"date\"])\n",
    "\n",
    "# Joining together the two datasets\n",
    "j_df=j_df.set_index([\"fips\",\"date\"]).sort_values([\"fips\",\"date\"])\n",
    "df=df.reset_index().sort_values([\"fips\",\"date\"])\n",
    "df=df.join(j_df,on=[\"fips\",\"date\"])\n",
    "\n",
    "# Reindexing everything for smooth loop\n",
    "des_df=des_df.set_index([\"state\",\"fips\",\"date\"]).sort_values([\"state\",\"fips\",\"date\"])\n",
    "df=df.set_index([\"state\",\"fips\",\"date\"]).sort_values([\"state\",\"fips\",\"date\"])\n",
    "\n",
    "# RIP performance\n",
    "for state, state_df in df.groupby(level='state'):\n",
    "    for fips, fips_df in state_df.groupby(level='fips'):\n",
    "        c_df=fips_df[\"m50_index\"]        \n",
    "        # If not enough data points, go fetch state data \n",
    "        #if np.sum(c_df.count())<c_df.size/10:\n",
    "        #    c_df=pd.merge(c_df,des_df[pd.isna(des_df[\"county\"])].loc[state][\"m50_index\"],how=\"left\",\n",
    "        #                  left_index=True,right_index=True,suffixes=('_c',''))[\"m50_index\"]\n",
    "        df.loc[(state,fips),\"m50_index\"]=c_df.interpolate().values\n",
    "    print(state, \"done !\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive plot\n",
    "Next we plot this data aggregated by state to be able to look for trends and correlations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with national totals\n",
    "initial_df = df.loc[:,'cases':'deaths'].groupby(['date']).aggregate(np.sum).reset_index()\n",
    "\n",
    "initial_data = {'date' : initial_df['date'],\n",
    "            'metric_1' : initial_df['cases'],\n",
    "            'metric_2' : initial_df['deaths']} \n",
    "        \n",
    "source = ColumnDataSource(initial_data)\n",
    "#print(source.data)\n",
    "\n",
    "#create initial plots\n",
    "#linear metric 1\n",
    "p1 = figure(title='COVID-19 data', x_axis_label='date', y_axis_label='Cumulative Cases',\\\n",
    "           plot_width=400, plot_height=300, x_axis_type=\"datetime\", y_axis_type='linear')\n",
    "line_p1 = p1.line(x='date', y='metric_1', source=source)\n",
    "\n",
    "panel_p1 = Panel(child=p1, title='linear')\n",
    "\n",
    "#log metric 1\n",
    "p2 = figure(title='COVID-19 data', x_axis_label='date', y_axis_label='Cumulative Cases',\\\n",
    "           plot_width=400, plot_height=300, x_axis_type=\"datetime\", y_axis_type='log')\n",
    "\n",
    "line_p2 = p2.line(x='date', y='metric_1', source=source)\n",
    "\n",
    "panel_p2 = Panel(child=p2, title='log')\n",
    "\n",
    "#panel metric 1\n",
    "panels_p = [panel_p1, panel_p2]\n",
    "\n",
    "#linear metric 2\n",
    "q1 = figure(title='COVID-19 data', x_axis_label='date', y_axis_label='Cumulative Cases',\\\n",
    "           plot_width=400, plot_height=300, x_axis_type=\"datetime\", y_axis_type='linear')\n",
    "\n",
    "line_q1 = q1.line(x='date', y='metric_2', source=source)\n",
    "\n",
    "panel_q1 = Panel(child=q1, title='linear')\n",
    "\n",
    "#log metric 2\n",
    "q2 = figure(title='COVID-19 data', x_axis_label='date', y_axis_label='Cumulative Cases',\\\n",
    "           plot_width=400, plot_height=300, x_axis_type=\"datetime\", y_axis_type='log')\n",
    "\n",
    "line_q2 = q2.line(x='date', y='metric_2', source=source)\n",
    "\n",
    "panel_q2 = Panel(child=q2, title='log')\n",
    "\n",
    "#panel metric 2\n",
    "panels_q = [panel_q1, panel_q2]\n",
    "    \n",
    "tabs_p = Tabs(tabs=panels_p)\n",
    "tabs_q = Tabs(tabs=panels_q)\n",
    "\n",
    "show(row(tabs_p,tabs_q), notebook_handle=True)\n",
    "\n",
    "def update_graph(level='national', state='California', county='Alameda',\\\n",
    "                 metric_1='cases', metric_2='deaths', \\\n",
    "                 method_1='cumulative',  method_2='cumulative', \\\n",
    "                 averaging=1, window=None, y_scale='linear', correlation=False, cobweb=False):    \n",
    "\n",
    "    #update county list dynamically\n",
    "    def update_counties(_):\n",
    "        counties = [e for e in df.loc[state_ls.value].reset_index()['county'].unique()]\n",
    "        county_ls.options = counties\n",
    "        county_ls.value = counties[0]\n",
    "        county = counties[0]\n",
    "    \n",
    "    def get_data(level, state, metric, method, averaging, window):\n",
    "        #fetch the data by national/state/county level\n",
    "        if level == 'national':\n",
    "            get_data = df[metric].groupby(['date']).aggregate(np.sum).reset_index()\n",
    "        if level == 'state':\n",
    "            get_data = df.loc[state, metric].groupby(['date']).aggregate(np.sum).reset_index()\n",
    "        if level == 'county':\n",
    "            get_data = df.loc[(state, county), metric].groupby(['date']).aggregate(np.sum).reset_index()\n",
    "        \n",
    "        #apply averaging / smoothing\n",
    "        get_data[metric] = get_data[metric].rolling(window=averaging, win_type=window, center=False).mean()#std=averaging)\n",
    "        \n",
    "        #apply edits\n",
    "        if method == 'difference':\n",
    "            get_data[metric] = get_data[metric].diff(periods=1).dropna()\n",
    "        if method == 'percent':\n",
    "            get_data[metric] = get_data[metric].pct_change(periods=1).dropna()\n",
    "        \n",
    "        return get_data\n",
    "    \n",
    "    state_ls.observe(update_counties)\n",
    "    \n",
    "    data_metric_1 = get_data(level, state, metric_1, method_1, averaging, window)\n",
    "    \n",
    "    data_metric_2 = get_data(level, state, metric_2, method_2, averaging, window)\n",
    "    \n",
    "    #find length of new data\n",
    "    length_patch = len(data_metric_1[metric_1])\n",
    "    #print(length_patch)\n",
    "    \n",
    "    #create updated data\n",
    "    stream = {'date' : data_metric_1['date'],\n",
    "             'metric_1' : data_metric_1[metric_1],\n",
    "             'metric_2' : data_metric_2[metric_2]}\n",
    "    \n",
    "    #replace existing data with new data\n",
    "    source.stream(stream, rollover=length_patch)  \n",
    "#     print(source.data)\n",
    "\n",
    "    def title(level, state, county, metric):\n",
    "        if level == 'national':\n",
    "            return f'COVID-19 Data: {level} {metric}'\n",
    "        if level == 'state':\n",
    "            return f'COVID-19 Data: {state} {metric}'\n",
    "        if level == 'county':\n",
    "            return f'COVID-19 Data: {county} {level} {metric}'\n",
    "        \n",
    "    #replace chart titles\n",
    "    p1.title.text = title(level,state,county,metric_1)\n",
    "    p1.yaxis.axis_label = f'{method_1} {metric_1}' \n",
    "    \n",
    "    p2.title.text = title(level,state,county,metric_1)\n",
    "    p2.yaxis.axis_label = f'{method_1} {metric_1}' \n",
    "    \n",
    "    q1.title.text = title(level,state,county,metric_2)\n",
    "    q1.yaxis.axis_label = f'{method_2} {metric_2}' \n",
    "    \n",
    "    q2.title.text = title(level,state,county,metric_2)\n",
    "    q2.yaxis.axis_label = f'{method_2} {metric_2}' \n",
    "    \n",
    "    if correlation == True:\n",
    "        corr_val = data_metric_1[metric_1].corr(data_metric_2[metric_2])\n",
    "        print(f'Correlation {corr_val}')\n",
    "        \n",
    "#     if cobweb == True:\n",
    "#         p.renderers.clear()\n",
    "#         step_p = p.step(x='metric_1', y='metric_1', source=source) \n",
    "#     else:\n",
    "#         p.renderers.clear()\n",
    "#         line_p = p.line(x='date', y='metric_1', source=source)\n",
    "    \n",
    "    push_notebook()\n",
    "#     print()\n",
    "\n",
    "#create widgets\n",
    "level = widgets.Dropdown(options=['national', 'state', 'county'], value='national', description='level')\n",
    "\n",
    "state_ls = widgets.Dropdown(options=[state for state in df.index.levels[0].values], value='California', description='state')\n",
    "\n",
    "county_ls = widgets.Dropdown(options=[county for county in df.loc[state_ls.value].reset_index()['county'].unique()], value='Alameda', description='county')\n",
    "\n",
    "metric_1 = widgets.Dropdown(options=[col for col in df.columns], value='cases', description='metric_1')\n",
    "\n",
    "metric_2 = widgets.Dropdown(options=[col for col in df.columns], value='deaths', description='metric_2')\n",
    "\n",
    "method_1 = widgets.Dropdown(options=['cumulative', 'difference', 'percent'], value='cumulative', description='method_1')\n",
    "\n",
    "method_2 = widgets.Dropdown(options=['cumulative', 'difference', 'percent'], value='cumulative', description='method_2')\n",
    "\n",
    "averaging = widgets.Dropdown(options=[k for k in range(1,10)], value=1, description='averaging')\n",
    "\n",
    "window = widgets.Dropdown(options=[None, 'triang', 'hamming', 'gaussian'], value=None, description='window')\n",
    "\n",
    "correlation = widgets.Checkbox(value=False, description='correlation' )\n",
    "\n",
    "#cobweb = widgets.Checkbox(value=False, description='cobweb' )\n",
    "\n",
    "#build widget UI\n",
    "widget_dict = {'level' : level, 'state' : state_ls, 'county' : county_ls ,\\\n",
    "               'metric_1' : metric_1, 'metric_2' : metric_2, \\\n",
    "               'method_1' : method_1, 'method_2' : method_2, \\\n",
    "               'averaging' : averaging,'window' : window, 'correlation': correlation} #, 'cobweb' : cobweb}\n",
    "\n",
    "col1 = widgets.VBox([level, state_ls, county_ls])\n",
    "col2 = widgets.VBox([metric_1, method_1])\n",
    "col3 = widgets.VBox([metric_2, method_2])\n",
    "col4 = widgets.VBox([averaging, window, correlation])\n",
    "\n",
    "ui = widgets.VBox([widgets.HBox([col1,col4]),widgets.HBox([col2,col3])])\n",
    "\n",
    "out = widgets.interactive_output(update_graph, widget_dict)\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot has several data manipulations. First, you can look at the data by state. Then you can control two plots. Each plot can show you a different metric in that state. For each metric, you can look at the value for that day (cumulative), the difference of the value on that day and the previous day (difference), and the percent change of the value on that day from the value the previous day (percent). The data can also be smoothed by using the averaging widget. It averages over the specified number of points, using the window (uniform, gaussian, etc) you can specify. The correlation between the two data sets plotted can also be computed, which may be more useful for comparing increases in cases to mobility, or the metrics you are interested in (warning: large correlation is not a sign of a causal relationship). Lastly the cobweb widget enables you to change how you plot the first graph so that it plots the number of cases today versus the number of cases tomorrow (interesting if you choose the metric 'cases' and the method 'difference' and compare different states: States that have 'flattened the curve' will show a line that walks up, then back down). All the data manipulations are performed using in-built pandas methods.\n",
    "\n",
    "It would be desireable to compare data between states and to get additional data about population density, Lockdown implementation, prevalence of underlying health conditions, and COVID testing so that more meaningful conclusions could be made. Unfortunately we do not have enough time to explore all of these opportunities.\n",
    "\n",
    "Credits: \n",
    "Geoffrey wrote all the code for merging data sets.\n",
    "Lorenzo wrote all the plotting code to integrate the data with all the data computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
